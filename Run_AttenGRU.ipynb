{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "from paddle.io import Dataset\n",
    "from paddle.static import InputSpec\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "from paddle.fluid.dygraph import Linear\n",
    "from paddle.fluid.dygraph import Layer, to_variable\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Scaler(object):\n",
    "    \"\"\"\n",
    "    Desc: Normalization utilities\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = 0.\n",
    "        self.std = 1.\n",
    "\n",
    "    def fit(self, data):\n",
    "        # type: (paddle.tensor) -> None\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Fit the data\n",
    "        Args:\n",
    "            data:\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(data)\n",
    "        self.std = np.std(data)\n",
    "\n",
    "    def transform(self, data):\n",
    "        # type: (paddle.tensor) -> paddle.tensor\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Transform the data\n",
    "        Args:\n",
    "            data:\n",
    "        Returns:\n",
    "            The transformed data\n",
    "        \"\"\"\n",
    "        mean = paddle.to_tensor(self.mean).type_as(data).to(data.device) if paddle.is_tensor(data) else self.mean\n",
    "        std = paddle.to_tensor(self.std).type_as(data).to(data.device) if paddle.is_tensor(data) else self.std\n",
    "        return (data - mean) / std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        # type: (paddle.tensor) -> paddle.tensor\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Restore to the original data\n",
    "        Args:\n",
    "            data: the transformed data\n",
    "        Returns:\n",
    "            The original data\n",
    "        \"\"\"\n",
    "        mean = paddle.to_tensor(self.mean) if paddle.is_tensor(data) else self.mean\n",
    "        std = paddle.to_tensor(self.std) if paddle.is_tensor(data) else self.std\n",
    "        return (data * std) + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "class WindTurbineDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Desc: Data preprocessing,\n",
    "          Here, e.g.    15 days for training,\n",
    "                        3 days for validation,\n",
    "                        and 6 days for testing\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path,\n",
    "                 filename='my.csv',\n",
    "                 flag='train',\n",
    "                 size=None,\n",
    "                 turbine_id=0,\n",
    "                 task='MS',\n",
    "                 target='Target',\n",
    "                 scale=True,\n",
    "                 start_col=3,       # the start column index of the data one aims to utilize\n",
    "                 day_len=24 * 6,\n",
    "                 train_days=15,     \n",
    "                 val_days=3,        \n",
    "                 test_days=6,       \n",
    "                 total_days=30      \n",
    "                 ):\n",
    "        super().__init__()  \n",
    "        self.unit_size = day_len    \n",
    "        if size is None:\n",
    "            self.input_len = self.unit_size   \n",
    "            self.output_len = self.unit_size    \n",
    "        else:\n",
    "            self.input_len = size[0]\n",
    "            self.output_len = size[1]\n",
    "        # initialization\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "        self.task = task\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.start_col = start_col\n",
    "        self.data_path = data_path\n",
    "        self.filename = filename\n",
    "        self.tid = turbine_id\n",
    "\n",
    "        # If needed, we employ the predefined total_size (e.g. one month)\n",
    "        self.total_size = self.unit_size * total_days   \n",
    "        #\n",
    "        self.train_size = train_days * self.unit_size   \n",
    "        self.val_size = val_days * self.unit_size\n",
    "        self.test_size = test_days * self.unit_size\n",
    "        \n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.data_path, self.filename))\n",
    "        \n",
    "        df_raw['Tmstamp'] = pd.to_datetime(df_raw['Tmstamp'])\n",
    "        df_raw.insert(0, 'hour',df_raw['Tmstamp'].dt.hour)\n",
    "        df_raw.insert(1, 'minute',df_raw['Tmstamp'].dt.minute)\n",
    "        del df_raw['Tmstamp']\n",
    "        del df_raw['Day']\n",
    "        df_raw.insert(3, 't_sin',np.sin((df_raw['hour']*6+df_raw['minute']/10)*2*np.pi/(24*6)))\n",
    "        df_raw.insert(4, 't_cos',np.cos((df_raw['hour']*6+df_raw['minute']/10)*2*np.pi/(24*6)))\n",
    "        \n",
    "        while df_raw.isna().sum().sum()>0:\n",
    "            df_raw = df_raw.fillna(axis=0, method='ffill')\n",
    "            df_raw = df_raw.fillna(axis=0,method='bfill')\n",
    "        \n",
    "        border1s = [self.tid * self.total_size,   \n",
    "                    self.tid * self.total_size + self.train_size - self.input_len,\n",
    "                    self.tid * self.total_size + self.train_size + self.val_size - self.input_len\n",
    "                    ]\n",
    "        border2s = [self.tid * self.total_size + self.train_size,\n",
    "                    self.tid * self.total_size + self.train_size + self.val_size,\n",
    "                    self.tid * self.total_size + self.train_size + self.val_size + self.test_size\n",
    "                    ]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        df_data = df_raw\n",
    "        if self.task == 'M':\n",
    "            cols_data = df_raw.columns[5:]   \n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.task == 'MS':\n",
    "            cols_data = df_raw.columns[5:]   \n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.task == 'S':\n",
    "            df_data = df_raw[[self.tid, self.target]]\n",
    "\n",
    "        # Turn off the SettingWithCopyWarning\n",
    "        pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "        df_data.replace(to_replace=np.nan, value=0, inplace=True)\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            \n",
    "            data = self.scaler.fit_transform(df_data.values)  \n",
    "        else:\n",
    "            data = df_data.values\n",
    "        \n",
    "        data = np.hstack((df_raw.values[:,3:5],data))\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]  \n",
    "        self.raw_data = df_data[border1 + self.input_len:border2]  \n",
    "\n",
    "    def get_raw_data(self):\n",
    "        return self.raw_data\n",
    "\n",
    "    def __getitem__(self, index):   \n",
    "        #\n",
    "        # Only for customized use.\n",
    "        # When sliding window not used, e.g. prediction without overlapped input/output sequences\n",
    "        if self.set_type >= 3:\n",
    "            index = index * self.output_len\n",
    "        #\n",
    "        # Standard use goes here.\n",
    "        # Sliding window with the size of input_len + output_len\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.input_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.output_len\n",
    "        \n",
    "        seq_x1 = self.data_x[s_begin:s_end]   \n",
    "        seq_x2 = self.data_y[r_begin:r_end,:2] \n",
    "        seq_y = self.data_y[r_begin:r_end]  \n",
    "        return [seq_x1,seq_x2], seq_y\n",
    "        #return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        # In our case, the sliding window is adopted, the number of samples is calculated as follows\n",
    "        if self.set_type < 3:\n",
    "            return len(self.data_x) - self.input_len - self.output_len + 1\n",
    "        # Otherwise, if sliding window is not adopted\n",
    "        return int((len(self.data_x) - self.input_len) / self.output_len)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_env():\n",
    "    # type: () -> dict\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Prepare the experimental settings\n",
    "    Returns:\n",
    "        The initialized arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Long Term Wind Power Forecasting')\n",
    "    ###\n",
    "    parser.add_argument('--data_path', type=str, default='./data/', help='Path to the data file')\n",
    "    parser.add_argument('--filename', type=str, default='wtbdata_245days.csv',\n",
    "                        help='Filename of the input data, change it if necessary')\n",
    "    parser.add_argument('--task', type=str, default='MS', help='The type of forecasting task, '\n",
    "                                                               'options:[M, S, MS]; '\n",
    "                                                               'M: multivariate --> multivariate, '\n",
    "                                                               'S: univariate --> univariate, '\n",
    "                                                               'MS: multivariate --> univariate')\n",
    "    parser.add_argument('--target', type=str, default='Patv', help='Target variable in S or MS task')\n",
    "    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='Location of model checkpoints')\n",
    "    parser.add_argument('--input_len', type=int, default=144, help='Length of the input sequence')\n",
    "    parser.add_argument('--output_len', type=int, default=288, help='The length of predicted sequence')\n",
    "    parser.add_argument('--start_col', type=int, default=3, help='Index of the start column of the meaningful variables')\n",
    "    parser.add_argument('--in_var', type=int, default=12, help='Number of the input variables')\n",
    "    parser.add_argument('--out_var', type=int, default=1, help='Number of the output variables')\n",
    "    parser.add_argument('--teacher_ratio', type=float, default=0.5, help='Number of the teacher ratio')\n",
    "    \n",
    "    parser.add_argument('--day_len', type=int, default=144, help='Number of observations in one day')\n",
    "    parser.add_argument('--train_size', type=int, default=200, help='Number of days for training')\n",
    "    parser.add_argument('--val_size', type=int, default=10, help='Number of days for validation')\n",
    "    parser.add_argument('--test_size', type=int, default=35, help='Number of days for testing')\n",
    "    parser.add_argument('--total_size', type=int, default=245, help='Number of days for the whole dataset')\n",
    "    parser.add_argument('--lstm_layer', type=int, default=2, help='Number of LSTM layers')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout')\n",
    "    parser.add_argument('--num_workers', type=int, default=5, help='#workers for data loader')\n",
    "    parser.add_argument('--train_epochs', type=int, default=100, help='Train epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for the input training data')\n",
    "    parser.add_argument('--patience', type=int, default=20, help='Early stopping patience')\n",
    "    parser.add_argument('--lr', type=float, default=2*1e-4, help='Optimizer learning rate')\n",
    "    parser.add_argument('--lr_adjust', type=str, default='type2', help='Adjust learning rate')\n",
    "    parser.add_argument('--model_selection', type=str, default='Atten_En_Decoder', help='Model selection:LSTM,En_DecoderGRE,Atten_EnDecoder_GRU')\n",
    "    parser.add_argument('--use_gpu', type=bool, default=True, help='Whether or not use GPU')\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='GPU ID')\n",
    "    # parser.add_argument('--use_multi_gpu', action='store_true', default=False, help='Use multiple gpus or not')\n",
    "    parser.add_argument('--capacity', type=int, default=1, help=\"The capacity of a wind farm, \"\n",
    "                                                                  \"i.e. the number of wind turbines in a wind farm\")\n",
    "    parser.add_argument('--turbine_id', type=int, default=0, help='Turbine ID')\n",
    "    parser.add_argument('--pred_file', type=str, default='./predict.py',\n",
    "                        help='The path to the script for making predictions')\n",
    "    parser.add_argument('--stride', type=int, default=1, help='The stride that a window adopts to roll the test set')\n",
    "    parser.add_argument('--is_debug', type=bool, default=False, help='True or False')\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "    settings = {\n",
    "        \"data_path\": args.data_path,\n",
    "        \"filename\": args.filename,\n",
    "        \"task\": args.task,\n",
    "        \"target\": args.target,\n",
    "        \"checkpoints\": args.checkpoints,\n",
    "        \"input_len\": args.input_len,\n",
    "        \"output_len\": args.output_len,\n",
    "        \"start_col\": args.start_col,\n",
    "        \"in_var\": args.in_var,\n",
    "        \"out_var\": args.out_var,\n",
    "        \"teacher_ratio\":args.teacher_ratio,\n",
    "        \"day_len\": args.day_len,\n",
    "        \"train_size\": args.train_size,\n",
    "        \"val_size\": args.val_size,\n",
    "        \"test_size\": args.test_size,\n",
    "        \"total_size\": args.total_size,\n",
    "        \"lstm_layer\": args.lstm_layer,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"num_workers\": args.num_workers,\n",
    "        \"train_epochs\": args.train_epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"patience\": args.patience,\n",
    "        \"lr\": args.lr,\n",
    "        \"lr_adjust\": args.lr_adjust,\n",
    "        \"model_selection\": args.model_selection,\n",
    "        \"capacity\": args.capacity,\n",
    "        \"turbine_id\": args.turbine_id,\n",
    "        \"pred_file\": args.pred_file,\n",
    "        \"stride\": args.stride,\n",
    "        \"is_debug\": args.is_debug\n",
    "    }\n",
    "    ###\n",
    "    # Prepare the GPUs\n",
    "    if paddle.device.is_compiled_with_cuda():\n",
    "        args.use_gpu = True\n",
    "        paddle.device.set_device('gpu:{}'.format(args.gpu))\n",
    "    else:\n",
    "        args.use_gpu = False\n",
    "        paddle.device.set_device('cpu')\n",
    "\n",
    "   \n",
    "    print(\"Experimental settings are: \\n{}\".format(str(settings)))\n",
    "    return settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = prep_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import time\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import random\n",
    "from paddle.io import DataLoader\n",
    "\n",
    "from Attention_GRU import Atten_En_Decoder\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    # type: (paddle.optimizer.Adam, int, dict) -> None\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Adjust learning rate\n",
    "    Args:\n",
    "        optimizer:\n",
    "        epoch:\n",
    "        args:\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # lr = args.lr * (0.2 ** (epoch // 2))\n",
    "    lr_adjust = {}\n",
    "    if args[\"lr_adjust\"] == 'type1':\n",
    "        # learning_rate = 0.5^{epoch-1}\n",
    "        lr_adjust = {epoch: args[\"lr\"] * (0.50 ** (epoch - 1))}\n",
    "    elif args[\"lr_adjust\"] == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 10: 1e-5, 16: 5e-6\n",
    "        }\n",
    "    if epoch in lr_adjust:\n",
    "        lr = lr_adjust[epoch]\n",
    "        optimizer.set_lr(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Experiment(object):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        The experiment to train, validate and test a model\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        # type: (dict) -> None\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            __init__\n",
    "        Args:\n",
    "            args: the arguments to initialize the experimental environment\n",
    "        \"\"\"\n",
    "        self.model = Atten_En_Decoder(args)\n",
    "        self.args = args\n",
    "\n",
    "    def get_model(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            the model\n",
    "        Returns:\n",
    "            An instance of the model\n",
    "        \"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def get_args(self):\n",
    "        # type: () -> dict\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Get the arguments\n",
    "        Returns:\n",
    "            A dict\n",
    "        \"\"\"\n",
    "        return self.args\n",
    "\n",
    "    def get_data(self, flag):\n",
    "        # type: (str) -> (WindTurbineDataset, DataLoader)\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            get_data\n",
    "        Args:\n",
    "            flag: train or test\n",
    "        Returns:\n",
    "            A dataset and a dataloader\n",
    "        \"\"\"\n",
    "        if flag == 'test':\n",
    "            shuffle_flag = False\n",
    "            drop_last = True\n",
    "        else:\n",
    "            shuffle_flag = True\n",
    "            drop_last = True\n",
    "        data_set = WindTurbineDataset(\n",
    "            data_path=self.args[\"data_path\"],\n",
    "            filename=self.args[\"filename\"],\n",
    "            flag=flag,\n",
    "            size=[self.args[\"input_len\"], self.args[\"output_len\"]],\n",
    "            task=self.args[\"task\"],\n",
    "            target=self.args[\"target\"],\n",
    "            start_col=self.args[\"start_col\"],\n",
    "            turbine_id=self.args[\"turbine_id\"],\n",
    "            day_len=self.args[\"day_len\"],\n",
    "            train_days=self.args[\"train_size\"],\n",
    "            val_days=self.args[\"val_size\"],\n",
    "            test_days=self.args[\"test_size\"],\n",
    "            total_days=self.args[\"total_size\"]\n",
    "        )\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=self.args[\"batch_size\"],\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=self.args[\"num_workers\"],\n",
    "            drop_last=drop_last\n",
    "        )\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        # type: () -> paddle.optimizer.Adam\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Get the optimizer\n",
    "        Returns:\n",
    "            An optimizer\n",
    "        \"\"\"\n",
    "        clip = paddle.nn.ClipGradByNorm(clip_norm=50.0)\n",
    "        model_optim = paddle.optimizer.Adam(parameters=self.model.parameters(),\n",
    "                                            learning_rate=self.args[\"lr\"],\n",
    "                                            grad_clip=clip)\n",
    "        return model_optim\n",
    "\n",
    "    @staticmethod\n",
    "    def get_criterion():\n",
    "        # type: () -> nn.MSELoss\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Use the mse loss as the criterion\n",
    "        Returns:\n",
    "            MSE loss\n",
    "        \"\"\"\n",
    "        criterion = nn.MSELoss(reduction='mean')  \n",
    "        return criterion\n",
    "\n",
    "    def process_one_batch(self, batch_x, batch_y):\n",
    "        # type: (list[paddle.tensor], paddle.tensor) -> (paddle.tensor, paddle.tensor)\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Process a batch\n",
    "        Args:\n",
    "            batch_x:\n",
    "            batch_y:\n",
    "        Returns:\n",
    "            prediction and ground truth\n",
    "        \"\"\"\n",
    "        batch_x1 = batch_x[0].astype('float32')\n",
    "        batch_x2 = batch_x[1].astype('float32')\n",
    "        batch_y = batch_y.astype('float32')\n",
    "        # If the task is the multivariate-to-univariate forecasting task,\n",
    "        # the last column is the target variable to be predicted\n",
    "        f_dim = -1 if self.args[\"task\"] == 'MS' else 0\n",
    "        batch_y = batch_y[:, -self.args[\"output_len\"]:, f_dim:].astype('float32')\n",
    "        \n",
    "        sample, att_list = self.model(batch_x1,batch_x2,batch_y)\n",
    "\n",
    "        sample = sample[..., :, f_dim:].astype('float32')\n",
    "        return sample, batch_y, att_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(settings)\n",
    "args = experiment.get_args()\n",
    "model = experiment.get_model()\n",
    "train_data,tran_loader= experiment.get_data(flag='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping(object):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        EarlyStopping\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=20, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.best_model = False\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path, tid):\n",
    "        # type: (nn.MSELoss, GruModel, str, int) -> None\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            Save current checkpoint\n",
    "        Args:\n",
    "            val_loss: the validation loss\n",
    "            model: the model\n",
    "            path: the path to be saved\n",
    "            tid: turbine ID\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.best_model = True\n",
    "        self.val_loss_min = val_loss\n",
    "        paddle.save(model.state_dict(), path + '/' + 'model_' + 'turbine{}'.format(str(tid)))  #这里是针对每一个turbine id训练一个模型吗？\n",
    "\n",
    "    def __call__(self, val_loss, model, path, tid):\n",
    "        # type: (nn.MSELoss, GruModel, str, int) -> None\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            __call__\n",
    "        Args:\n",
    "            val_loss: the validation loss\n",
    "            model: the model\n",
    "            path: the path to be saved\n",
    "            tid: turbine ID\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path, tid)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.best_model = False\n",
    "            self.save_checkpoint(val_loss, model, path, tid)\n",
    "            #if self.counter >= self.patience:\n",
    "                #self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.update_hidden = True\n",
    "            self.save_checkpoint(val_loss, model, path, tid)\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val(experiment, data_loader, criterion):\n",
    "    # type: (Experiment, DataLoader, Callable) -> np.array\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Validation function\n",
    "    Args:\n",
    "        experiment:\n",
    "        data_loader:\n",
    "        criterion:\n",
    "    Returns:\n",
    "        The validation loss\n",
    "    \"\"\"\n",
    "    validation_loss = []\n",
    "    for i, (batch_x, batch_y) in enumerate(data_loader):\n",
    "        sample, true,att = experiment.process_one_batch(batch_x, batch_y)\n",
    "        loss = criterion(sample, true)\n",
    "        validation_loss.append(loss.item())\n",
    "    validation_loss = np.average(validation_loss)\n",
    "    return validation_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fix_seed = 3407\n",
    "random.seed(fix_seed)\n",
    "paddle.seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "def traverse_wind_farm(method, params, model_path, flag='train'):\n",
    "    # type: (Callable, dict, str, str) -> list\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Traverse the turbines in a wind farm on by one\n",
    "    Args:\n",
    "        method: the method for training or testing on the records of one turbine\n",
    "        params: the arguments initialized\n",
    "        model_path: the folder name of the model\n",
    "        flag: 'train' or 'test'\n",
    "    Returns:\n",
    "        Predictions (for test) or None\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    start_time = time.time()\n",
    "    for i in range(params[\"capacity\"]):\n",
    "        params[\"turbine_id\"] = i\n",
    "        exp = Experiment(params)\n",
    "        if 'train' == flag:\n",
    "            print('>>>>>>> Training Turbine {:3d} >>>>>>>>>>>>>>>>>>>>>>>>>>\\n'.format(i))\n",
    "            method(exp, model_path, is_debug=params[\"is_debug\"])\n",
    "        elif 'test' == flag:\n",
    "            print('>>>>>>> Forecasting Turbine {:3d} >>>>>>>>>>>>>>>>>>>>>>>>>>\\n'.format(i))\n",
    "            res = method(exp, model_path)\n",
    "            responses.append(res)\n",
    "        else:\n",
    "            pass\n",
    "        paddle.device.cuda.empty_cache()\n",
    "        if params[\"is_debug\"]:\n",
    "            end_time = time.time()\n",
    "            print(\"Elapsed time for {} turbine {} is {} secs\".format(\"training\" if \"train\" == flag else \"predicting\", i,\n",
    "                                                                     end_time - start_time))\n",
    "            start_time = end_time\n",
    "    if 'test' == flag:\n",
    "        return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_val(experiment, model_folder, is_debug=False):\n",
    "    # type: (Experiment, str, bool) -> None\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Training and validation\n",
    "    Args:\n",
    "        experiment:\n",
    "        model_folder: folder name of the model\n",
    "        is_debug:\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    args = experiment.get_args()\n",
    "    model = experiment.get_model()\n",
    "    train_data, train_loader = experiment.get_data(flag='train')\n",
    "    val_data, val_loader = experiment.get_data(flag='val')\n",
    "\n",
    "    path_to_model = os.path.join(args[\"checkpoints\"], model_folder)\n",
    "    if not os.path.exists(path_to_model):\n",
    "        os.makedirs(path_to_model)\n",
    "\n",
    "    time_now = time.time()\n",
    "    early_stopping = EarlyStopping(patience=args[\"patience\"], verbose=True)\n",
    "    model_optim = experiment.get_optimizer()\n",
    "    criterion = Experiment.get_criterion()\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    for epoch in range(args[\"train_epochs\"]):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            sample, truth, at_list = experiment.process_one_batch(batch_x, batch_y)\n",
    "            \n",
    "            loss = criterion(sample, truth)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            model_optim.minimize(loss)\n",
    "            model_optim.step()\n",
    "        val_loss = val(experiment, val_loader, criterion)\n",
    "        print('Epoch: {}, Train loss average'.format(epoch), np.mean(np.array(train_loss)))\n",
    "\n",
    "        if is_debug:\n",
    "            train_loss = np.average(train_loss)\n",
    "            epoch_end_time = time.time()\n",
    "            print(\"Epoch: {}, \\nTrain Loss: {}, \\nValidation Loss: {}\".format(epoch, train_loss, val_loss))\n",
    "            print(\"Elapsed time for epoch-{}: {}\".format(epoch, epoch_end_time - epoch_start_time))\n",
    "            epoch_start_time = epoch_end_time\n",
    "\n",
    "        # Early Stopping if needed\n",
    "        early_stopping(val_loss, model, path_to_model, args[\"turbine_id\"])\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopped! \")\n",
    "            break\n",
    "        adjust_learning_rate(model_optim, epoch + 1, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    settings = prep_env()\n",
    "    #\n",
    "    # Set up the initial environment\n",
    "    # Current settings for the model\n",
    "    cur_setup = '{}_t{}_i{}_o{}_ls{}_train{}_val{}_teacher{}_model{}'.format(\n",
    "        settings[\"turbine_id\"], settings[\"task\"], settings[\"input_len\"], settings[\"output_len\"], settings[\"lstm_layer\"],\n",
    "        settings[\"train_size\"], settings[\"val_size\"],settings[\"teacher_ratio\"], settings[\"model_selection\"]  #计划对每个tid保存一个模型，明天在这里加上id\n",
    "    )\n",
    "    traverse_wind_farm(train_and_val, settings, cur_setup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
